# -*- coding: utf-8 -*-
"""Question2_TASK1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQ3Y6VbQ55LWD4B77NoQoq-08zMt6duE
"""

!wget -O UrbanSound8K.zip https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz?download=1
!mkdir UrbanSound8K
!tar -xvzf UrbanSound8K.zip -C UrbanSound8K

"""### Verifying the download"""

import os

dataset_dir = 'UrbanSound8K/UrbanSound8K/audio'
# List the files and subdirectories to check the structure
os.listdir(dataset_dir)

import os
import torch
import torchaudio
import torchaudio.transforms as T
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import random

"""### Setting GPU"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

Dataset_path = "/content/UrbanSound8K/UrbanSound8K/audio/"
metadata_file= "/content/UrbanSound8K/UrbanSound8K/metadata/UrbanSound8K.csv"
df = pd.read_csv(metadata_file)
df.head()

window_types = {
    "hann": torch.hann_window,
    "hamming": torch.hamming_window,
    "rectangular": lambda x: torch.ones(x)
}

def process_audio(file_path, window_type, n_fft=2048, hop_length=512):
    waveform, sample_rate = torchaudio.load(file_path)
    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0)  # Convert to mono

    window = window_types[window_type](n_fft)
    spectrogram = torch.stft(waveform, n_fft=n_fft, hop_length=hop_length, window=window, return_complex=True)
    magnitude = torch.abs(spectrogram)
    return 20 * torch.log10(magnitude + 1e-6)

all_files = []
for fold in os.listdir(Dataset_path):
    fold_path = os.path.join(Dataset_path, fold)
    if os.path.isdir(fold_path):
        all_files.extend([os.path.join(fold_path, f) for f in os.listdir(fold_path) if f.endswith(".wav")])

# Select random audio samples
num_samples = 5  # Number of random samples to visualize
random_samples = random.sample(all_files, num_samples)

class_labels = {
    0: "Air Conditioner",
    1: "Car Horn",
    2: "Children Playing",
    3: "Dog Bark",
    4: "Drilling",
    5: "Engine Idling",
    6: "Gun Shot",
    7: "Jackhammer",
    8: "Siren",
    9: "Street Music",
}

plt.figure(figsize=(15, 3 * num_samples))
for idx, sample_file in enumerate(random_samples):
    file_name = os.path.basename(sample_file)
    class_id = int(file_name.split("-")[1])  # Extract classID from filename
    class_name = class_labels.get(class_id, "Unknown")
    for i, (name, _) in enumerate(window_types.items()):
        spectrogram = process_audio(sample_file, name)
        plt.subplot(num_samples, 3, idx * 3 + i + 1)
        plt.imshow(spectrogram.numpy().T, origin='lower', aspect='auto', cmap='viridis')
        plt.title(f"Sample {idx+1}: {class_name} ({name.capitalize()} Window)")
        plt.xlabel("Time")
        plt.ylabel("Frequency")

plt.tight_layout()
plt.show()

# Load audio file using torchaudio
file_path = "/content/UrbanSound8K/UrbanSound8K/audio/fold5/100032-3-0-0.wav"
waveform, sampling_rate = torchaudio.load(file_path)

# Convert stereo to mono (if applicable)
if waveform.shape[0] > 1:
    waveform = waveform.mean(dim=0, keepdim=True)

# Define the Mel Spectrogram transformation
mel_spectrogram = T.MelSpectrogram(
    sample_rate=sampling_rate,
    n_fft=2048,  # Equivalent to librosa default
    hop_length=512,  # Default hop length in librosa
    n_mels=128  # Default number of mel bands in librosa
)

# Compute Mel spectrogram
mel_spec = mel_spectrogram(waveform)

# Print shape
print(mel_spec.shape)  # Shape: (1, 128, TimeFrames)

features = []
labels = []

def parser(row):
    for i in range(len(df)):  # Use `len(df)` instead of hardcoding 8732
        file_name = os.path.join(Dataset_path, f"fold{df['fold'][i]}", df["slice_file_name"][i])

        # Load audio using torchaudio
        waveform, sample_rate = torchaudio.load(file_name)

        # Convert stereo to mono if necessary
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)

        # Compute Mel Spectrogram using PyTorch
        mel_transform = T.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=2048,
            hop_length=512,
            n_mels=128
        )
        mels = mel_transform(waveform)

        # Take mean across time axis (equivalent to `np.mean(..., axis=0)`)
        mels = torch.mean(mels, dim=2).squeeze()  # Shape: (128,)

        # Store feature and label
        features.append(mels.numpy())
        labels.append(df["classID"][i])

    return [features,labels]

temp = parser(df)

X = np.vstack(temp[0])
Y = np.array(temp[1])

num_classes = len(set(Y))  # Number of unique classes
Y_one_hot = np.eye(num_classes)[Y]  # One-hot encoding using identity matrix

print(X.shape, Y_one_hot.shape)  # Check the shapes of the feature matrix and labels

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)

X_train = X_train.reshape(6549, 16, 8, 1)
X_test = X_test.reshape(2183, 16, 8, 1)

input_dim = (16, 8, 1)

# Define the CNN model
# Define the CNN model
class CNNModel(nn.Module):
    def __init__(self, input_dim):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=input_dim[0], out_channels=64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        # Adjust the kernel size or stride to avoid output size being too small
        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # Changed to (2, 1) to prevent dimension becoming 0
        self.dropout = nn.Dropout(0.1)
        # Adjust the calculation for the fully connected layer input size based on the new pooling
        self.fc1 = nn.Linear(128 * input_dim[1] // 2 * input_dim[2], 1024)
        self.fc2 = nn.Linear(1024, 10)

    def forward(self, x):
        x = torch.tanh(self.conv1(x))
        x = self.pool(x)
        x = torch.tanh(self.conv2(x))
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = torch.tanh(self.fc1(x))
        x = self.fc2(x)
        return x

# Assuming input_dim = (channels, height, width)
input_dim = (16,8,1)  # For example, if input images are RGB and 32x32
model = CNNModel(input_dim)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()  # For categorical classification
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Convert X_train, Y_train, X_test, Y_test to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)  # CrossEntropyLoss expects labels in long format
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)

# Training loop
epochs = 90
batch_size = 50
for epoch in range(epochs):
    model.train()  # Set model to training mode
    for i in range(0, len(X_train), batch_size):
        # Get batch data
        X_batch = X_train_tensor[i:i+batch_size]
        Y_batch = Y_train_tensor[i:i+batch_size]

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(X_batch)

        # Compute loss
        loss = criterion(outputs, Y_batch)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # Validation
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        outputs = model(X_test_tensor)
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == Y_test_tensor).sum().item() / len(Y_test_tensor) * 100

    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')

# Model Summary (Manually print the model architecture)
print(model)

# Perform predictions (forward pass on test set)
model.eval()  # Set model to evaluation mode
with torch.no_grad():  # No need to compute gradients
    predictions = model(X_test_tensor)
    _, preds = torch.max(predictions, 1)  # Get class with highest probability

# Compute accuracy
correct = (preds == Y_test_tensor).sum().item()
total = Y_test_tensor.size(0)
accuracy = 100 * correct / total
print(f"Accuracy: {accuracy:.2f}%")

# Save predictions to CSV
preds = preds.numpy()  # Convert PyTorch tensor to NumPy array
result = pd.DataFrame(preds, columns=['Predictions'])
result.to_csv("UrbanSound8kResults.csv", index=False)  # Save predictions to CSV

"""# SVM"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler

import seaborn as sns

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)

X_train.shape, Y_train.shape, X_test.shape, Y_test.shape

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

svc=SVC(C=1000.0)


# fit classifier to training set
svc.fit(X_train,Y_train)


# make predictions on test set
y_pred=svc.predict(X_test)


# compute and print accuracy score
print('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(Y_test, y_pred)))

confusion_matrix(Y_test, y_pred)

print(classification_report(Y_test, y_pred))

"""# Random Forest Classifier

"""

forest = RandomForestClassifier()

# fit classifier to training set
forest.fit(X_train, Y_train)

forest_pred = forest.predict(X_test)

print(classification_report(Y_test, forest_pred))

confusion_matrix(Y_test, forest_pred)

plt.figure(figsize = (12, 10))
sns.heatmap(confusion_matrix(Y_test, forest_pred),
            annot = True, linewidths = 2, fmt="d",
            xticklabels = class_name,
            yticklabels = class_name)
plt.show()